---
title: 'MixDiff: a framework to reconstruct dataset with missing or erroneous data
  for outbreaks intervention'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Method

## Pathways

There exist four different pathays depending if the case has been hospitalised 
and/or survived. The date associated with the four health seeking pathways are 
given in the following table: 

|                  | Onset | Hospitalisation | Death | Discharge | Report |
|------------------|-------|-----------------|-------|-----------|--------|
| $HD$             |   X   |        X        |   X   |           |    X   |
| $H\bar{D}$       |   X   |        X        |       |     X     |    X   |
| $\bar{H}D$       |   X   |                 |   X   |           |    X   |
| $\bar{H}\bar{D}$ |   X   |                 |       |           |    X   |

## Notations

For each individual $i$, we denote $y_i^k$ $(k=1, \ldots, n_{g_i})$ the observed 
data for that individual. 
The number of observed data $n_{g_i}$ can depend on characteristics of the 
individual, defined by a grouping so that individual $i$ is in group $g_i$. 

Here, we consider observed data as being epidemiologically relevant dates, which 
depend on the path of the individual (see table above): if the individual was 
hospitalised and died ($g_i=HD$) or was hospitalised and did not die 
($g_i=H\bar{D}$), $n_{g_i}=4$; if the individual was not hospitalised and died 
($g_i=\bar{H}D$), $n_{g_i}=3$; if the individual was not hospitalised and died 
($g_i=\bar{H}\bar{D}$), $n_{g_i}=2$.

We are interested in inferring the timing of the different steps of the 
healthcare pathways depending on the group $g_{i}$. To fully describe the 
healthcare pathway, we need to define the onset of the disease (no other date 
should be anterior) and then $n_{g_i}-1$ delays to reach the $n_{g_i}-1$ 
remaining enpoints. Any pairs of points can be chosen, providing that there 
exists a path from the origin to the endpoints. For example, in the case of the 
group $HD$, (onset -> hospitalisation, hospitalisation -> death, onset -> report) 
is a valid representation, while (onset -> hospitalisation, 
hospitalisation -> Death, onset -> report)
$\Delta_i^k$ the delay taken by individual $i$ for the $k$th delay distribution. 

We assume that each data (here date) can have been recorded with error, or can 
be missing in the observations. Hence we introduce augmented data $D_i^k$ 
$(k=1, \ldots, n_{g_i})$, where $D_i^k$ is the true data corresponding to the 
observed data $y_i^k$. 

## Likelihood

### Observation level

The data from the healthcare pathways can be missing or recorded, and, if 
recorded, can be correct or erroneous. To each observed data entry $y_i^k$ is 
thus associated an indicator function $E_i^k \in \{-1,0,1 \}$ (missing, recorded 
and no error, recorded and erroneous). We have
$$P(E_i^k=1|E_i^k \neq -1)= Bernoulli(\zeta)$$
with $\zeta$ the error probability. The likelihood associated with the observed 
data $y_i^k$ conditional on the true data $D_i^k$ is thus:
$$P(y_i^k|D_i^k,E_i^k=0)=\delta_{y_i^k,D_i^k}$$
$$P(y_i^k|D_i^k,E_i^k=1)=h_{\theta_h}\left(D_i^k\right)$$
$$P(y_i^k|D_i^k,E_i^k=-1)=1$$

Here we assume that any date between $T$ and $T_{0}$ are equally likely to be 
recorded and thus $h_{\theta_h}\left(D_i^k\right)=\frac{1}{T-T_{0}}$.

NOTE: discuss what $h_{\theta_h}$ should be; could include conditions such as 
error on onset date > error on death date. Also think about the space of possible 
errors - if wider than h is smaller and moves towards E=1 might be difficult to 
achieve. What happens if T_0 is ridiculously early or T ridiculously late in the 
dataset - this will affect the likelihood. 

??? Probably need a prior for p(D, E), which will include the 
$$P(E_i^k=1|E_i^k \neq -1)= Bernoulli(\zeta)$$ but also requires a p(D) ?

### Difference level

This level describes the difference between two observations: 
for $k\geq 2$, $P\left(D_i^k|D_i^{k-1}, g_i\right) = f_{k-1, k}^{g_i}\left(D_i^{k}-D_i^{k-1}\right)$. 

We use discretised gamma distributions for $f_{k-1, k}^{g_i}$ with mean and 
coefficient of variation (before discretisation) being noted respectively 
$\mu_{k-1, k}^{g_i}$ and $CV_{k-1, k}^{g_i}$. 

<!--In our example, we assume that the distribution of the delay between exposure 
and onset $f_{E, O}^{g_i}=f_{E, O}$ does not depend on the group - NOT RELEVANT 
FOR NOW AS NOT CONSIDERING EXPOSURE DATE -->

### Full posterior distribution

The joint posteior distribution of parameters and augmented data given observed 
data is:

$\begin{aligned}
P\left(\zeta, \mu, CV, D, E | g, y\right) \propto& P\left(\zeta, \mu, CV, D, E, g, y\right) \\
  \propto& P\left(y | D, E, \zeta, \mu, CV, g\right) P\left(E | \zeta, D, \mu, CV, g\right) P\left(D | \zeta, \mu, CV, g\right) P\left(\zeta\right) P\left(\mu\right) P\left(CV\right)\\
    \propto& P\left(y | D, E\right) P\left(E | \zeta\right) P\left(D | \zeta, \mu, CV, g\right)P\left(\zeta\right) P\left(\mu\right) P\left(CV\right)
\end{aligned}$

where $P\left(D | \zeta, \mu, CV, g\right) \propto \prod_i \prod_{k=2}^{n_{g_i}} P\left( D_i^k | D_i^{k-1}, \mu, CV, g \right)$ (assuming a uniform prior for $D_i^1$). 

## Inference

Inference is made in a Bayesian framework using Markov Chain Monte Carlo (MCMC). 

### Priors

We use an informative beta prior for $\zeta$, and uninformative flat exponential 
priors for $\mu$ and $CV$, the mean and coefficient of variation of all delays. 

### Initialisation of the Markov Chain

We first initialise the augmented data as follows. 
We start by setting $E = -1$ for all missing dates. 
We then assume that all other dates are correctly recorded i.e. $D = y$ and 
$E = 0$.
We define a maximum delay (100 days by default), so that delays which are either
strictly negative or above this maximum delay are considered unrealistic.
For each group, we identify dates involved in any unrealistic delay. 
For each unrealistic delay identified, we decide on which date involved in this 
delay is most likely to be incorrectly recorded with the following priority 
rules: dates involved in several unrealistic delays are more likely to be 
recorded erroneously; dates furthest away from the median of all recorded dates
are more likely to be erroneous. Dates identified as erroenously recorded
through this process are first set to be incorrectly recorded ($E = 1$) and we 
temporarily assume that the corresponding dates are $D = NA$.
This process of identification of erroneously recorded dates is repeated 
recursively until no more unrealistic delays can be identified. 
The true dates $D$ correspondigng to the observations identified as erroneous, 
as well as those corresponding to the missing observations
are then drawn (one at a time) uniformly among a plausible range of dates. 
For each group and each date, such range of dates is defined from the 
combination of 
i) the graph (which defines an ordering between dates), 
ii) the set of observed dates for this individual (which combined with the graph 
may impose some bounds on the date under consideration), and 
ii) setting an absolute minimum and maximum range for all dates (by default the 
minimum and maximum observed dates across the entire dataset). 

We then initialise the parameters of the model conditional on the augmented data
as follows. 
The probability of error $\zeta$ is set to a given value between 0 and 1 
(0.1 by default); this value does not matter as zeta is then updated using a 
Gibbs sampler. 
The mean and coefficient of variation of each delay are set to the observed mean 
and coefficient of variation for the corresponding augmented dates $D$. 

### Moves

At each iteration of the MCMC, the following movements are performed: 

1) Move a fraction of the dates $D$. For each group of individuals and for each 
date in that group, a given fraction (1/10 by default) of cases are randomly 
selected. For each of them, the corresponding date $D$ is updated using an 
independant Metropolis algorithm, with a candidate value drawn in the marginal 
mixture distribution of posterior delays this particular date is directly 
involved in (i.e. is directly connected to in the underlying timeline graph for 
that group). 
The error status for that date, $E$, is automatically updated accordingly. 
We then accept or reject the candidate pair $D$ and $E$. The acceptance 
probability accounts for the asymetry in the proposal distribution. 

2) Move a fraction of the error statuses $E$. For each group of individuals and 
for each date in that group, a given fraction (1/10 by default) of cases are 
randomly selected. For each of them, the corresponding error status $E$, if the 
corresponding recorded date $y$ is not missing ($E \neq -1$), we update $E$ 
using a Metropolis algorithm with the candidate value calculated as 
$E_{proposed} = 1 - E_{current}$.
If $E_{proposed} = 0$ then the corresponding date  $D$ is automatically updated 
so that $D = y$. If $E_{proposed} = 1$ then the corresponding date $D$ is 
updated with a candidate value drawn in the marginal mixture distribution of 
posterior delays this particular date is directly involved in (i.e. is directly 
connected to in the underlying timeline graph for that group), excluding the 
value $D = y$. 
We then accept or reject the candidate pair $D$ and $E$. The acceptance 
probability accounts for the asymetry in the proposal distribution for $D$. 

3) Swap error statuses $E$ for a fraction of individuals. First, for each group,
we identify all individuals who, in the current state of the augmented data 
chain, have at least one erroneous ($E = -1$) and one non erroneous ($E = 0$) 
dates. For each of these individuals (one at a time), 
[COULD PERHAPS DO ONLY A FRACTION HERE?] 
we then propose to change all error statuses for non missing 
dates using a Matropolis algorithm. 
First, we propose to update all currently
incorrectly recorded dates ($E = 1$) to a be correctly recorded ($E = 0$). The 
corresponding dates $D$ are then updated so that $D = y$. 
Second, if the individual has any missing dates ($E = -1$), we update the 
corresponding dates $D$ one at a time by drawing a candidate value in the 
marginal mixture distribution of posterior delays this particular date is 
directly involved in (i.e. is directly connected to in the underlying timeline 
graph for that group), but ignoring delays involving dates which were either 
i) correctly recorded ($E = 0$) as we will switch them to $E = 1$ just after, 
and therefore don't want to rely on their corresponding $D$ values or 
ii) missing and have not yet been updated, as the corresponding current $D$ 
values dependent on the dates in i). 
Finally, we propose to update all currently
correctly recorded dates ($E = 0$) to a be incorrectly recorded ($E = 1$). 
We update the corresponding dates $D$ one at a time by drawing a candidate value 
in the marginal mixture distribution of posterior delays this particular date is 
directly involved in (i.e. is directly connected to in the underlying timeline 
graph for that group), but as before, ignoring delays involving dates which were
currently correctly recorded ($E = 0$) and have not yet been updated to $E = 1$.
We then accept or reject the candidate set of dates $D$ and error statuses $E$
for that individual. The acceptance probability accounts for the asymetry in 
the proposal distribution. 
[FOR THE ABOVE TO WORK I THINK WE NEED TO HAVE A FULLY CONNECTED GRAPH. 
IF THAT'S NOT THE CASE, THE SWAPPING NEEDS TO HAPPEN INDEPENDENTLY ON EACH
PORTION OF THE GRAPH]

4) Move the probability of error $\zeta$. This is done using a Gibbs sampler, 
drawing from the marginal Beta distribution. 

5) Move the mean delays. For each group and each delay in that group, we update
the mean corresponding delay $\mu$ using a Metropolis algorithm with Lognormal 
proposal. 

6) Move the coeeficient of variation of the delays. For each group and each 
delay in that group, we update the mean corresponding delay $CV$ using a 
Metropolis algorithm with Lognormal proposal. 

## Simulation study

I order to assess the ability of our model to correctly estimate delay
distributions while at the same time inferring missing dates and identifying and
correcting erroneous dates, we performed an extensive simulation study. 

The simulation study was designed to mimick an Ebola like dataset. We considered 
4 groups of individuals defined by their combination of hospitalisation status
(hospitalised or not) and their outcome (dead / alive). For each group, we 
considered a timeline which included date of symtpom onset and date of report 
(for all groups), date of hospitalisation (for those hospitalised), date of 
discharge (for those who were hospitalised and survived) and date of death (for
 those who died). Relationship between these dates were defined through the 
 directed acyclic graphs (DAGs) shown in Figure XXX, where a directed edge 
 between two dates defines both an order between the edge (an edge from date 1
 to date 2 indicating that date 2 is after date 1) as well as an 
 epidemiologically relevant delay (from date 1 to date 2 in the previous 
 example). 

We defined different simulation scenarios as variations of a baseline scenario. 
In the baseline scenario, we simulated 100 individuals in each of the 4 groups. 
For each individual, a date of symptom onset was randomly drawn uniformly 
between 1st January and 31st December 2014. Delays defined by the DAGs (or 
timeline) for the relevant group were then were then drawn from Gamma 
distributions with parameters reflecting the epidemiological delays estimated
for Ebola in the 2013-16 West African epidemic 
[REF https://www.nejm.org/doi/pdf/10.1056/NEJMc1414992?articleTools=true], 
shown in table XXX. 

Delay                         | Mean (days)   | Sd (days)     | Groups using delay    
----------------------------- | ------------- | ------------- | ----------------------------------------------
Onset to hospitalisation      | 5.0           | 4.4           | $HD$, $H\bar{D}$  
Hospitalisation to discharge  | 11.2          | 7.2           | $H\bar{D}$  
Hospitalisation to death      | 4.3           | 4.0           | $HD$  
Onset to report               | 5.5           | 5.2           | $HD$, $H\bar{D}$, $\bar{H}D$, $\bar{H}\bar{D}$

TO BE CONTINUED

[THINK ABOUT WHETHER WE COULD HAVE SEVERAL ARROWS WITH THE SAME DATE 2?? 
IF SO WILL NEED TO ACCOUNT FOR THIS IN THE SIMULATION AS AT THE MOMENT WE ASSUME
EACH DATE IS DEFINED AS THE TIP OF THE ARROW OF A SINGLE OTHER DATE - WHICH
MAKES SIMULATION EASIER AS CAN DRAW FROM A SINGLE GAMMA RATHER THAN A MIXTURE]

## Performance of the model on simulated data

TO DESCRIBE

### Estimation of the delay distributions

TO DESCRIBE

### Detection of erroneous data entries

TO DESCRIBE

### Estimation of the missing dates

TO DO ?!?! AND DESCRIBE

## Error model

* We assume that the data entries are correct typewise, i.e. they are actual date (so "45/10/2012" is not an option)

* We model the potential error generation by a multinomial with five probabilities corresponding to:
1. Internal swap (transposition), the data entry person (DEP) swap the two figures of the month or day (e.g. 12 instead of 21), probability $\epsilon_{w}$
2. External swap (transposition), day and month subfields are swapped, probability $\epsilon_{b}$
3. Neighbouring typo (transcription), DEP mistype one digit for one nearby on the keyboard, +/- 1 position on the keyboard - to start with, probability $\epsilon_{n}$
4. Random typo (transcription), DEP replace one digit by a non neighbour one while the date remain valid, probability $\epsilon_{R}$
5. Unspecified error, any other error not defined above, probability $1- \sum\limits_{X=\{w, b, n, r\}} \epsilon_{X}$

## Website

* We have 5 possible ways of proposing a date
1. Plain writing e.g. "Thursday 24th June 2014"
2. American way e.g. June 24th 2014
3. Month of calendar (image) with a date circle in rate
4. Relative day, e.g, "last Monday" [I DON'T THINK WE DID THIS BUT COULD ADD TO THE DISCUSSION]
5. Use image database of handwriting training set to generate a "handwritten" data in the right format (reading error)

Regression to see if particular entries are associate with particular error (or simply more errors)

# Plan d'attaque

* Essential
2. Adaptive sd for proposals - independently or jointly?  - Anne (& Marc if jointly)
3. Speed up - put some code in C - Anne - Marc
4. Coder le model d'erreur & Write a likelihood based on recorded date and a proposed date - Marc
5. Retrieve original uncleaned ebola dataset - Anne
8. How many of the typo challenge errors are still true dates? (Marc)
9. Preprocessing tool ? To capture wrong separators and clear m/d swap. If not a date, can we propose another date and increase the prior on the error / decrease the belief in that date? Marc
10. Marc to review code - does it run, is it understandable? 
11. Marc to create shared folder for paper - Text, figures etc.

* Non essential
1. Explorer simulations en parallele (Anne)
2. Write some code to restart from last point of the chain (Anne) use MCMCres$theta_chain[[length(MCMCres$theta_chain)]] & MCMCres$aug_dat_chain[[length(MCMCres$aug_dat_chain)]]
3. ??? Where we use ncol(curr_aug_dat$D[[g]]), check this as I think it may need to be defined from index_dates rather than from D (Anne)
5. Write some tests for the R package

* Done
1. Have an option to use exact DistrGamma or approximate which is faster (Anne) --> did this but in the end the exact one seems faster so removed from code
2. Try to speed up DistrGamma by vectorising lines (Anne) --> tried but wasn't faster so removed
3. Allow different sd for proposal for different delay means and sds - Anne - DONE
4. do we indeed want to update zeta after each D_i move? maybe not useful? (Anne) - Indeed I commented this out as didn't make any difference (but didn't make much difference computing time-wise!)