---
title: "MixDiff: a framework to reconstruct dataset with missing or erroneous data for outbreaks intervention"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Method

##Pathways

There exist four different pathays depending if the case has been hospitalised and/or survived. The date associated with the four health seeking pathways are given in the following table: 

|                  | Onset | Hospitalisation | Death | Discharge | Report |
|------------------|-------|-----------------|-------|-----------|--------|
| $HD$             |   X   |        X        |   X   |           |    X   |
| $H\bar{D}$       |   X   |        X        |       |     X     |    X   |
| $\bar{HD}$       |   X   |                 |   X   |           |    X   |
| $\bar{H}\bar{D}$ |   X   |                 |       |           |    X   |

## Notations

For each individual $i$, we denote $y_i^k$ $(k=1, \ldots, n_{g_i})$ the observed data for that individual. 
The number of observed data $n_{g_i}$ can depend on characteristics of the individual, defined by a grouping so that individual $i$ is in group $g_i$. 

Here, we consider observed data as being epidemiologically relevant dates, which depend on the path of the individual (see table above): if the individual was hospitalised and died ($g_i=HD$) or was hospitalised and did not die ($g_i=H\bar{D}$), $n_{g_i}=4$; if the individual was not hospitalised and died ($g_i=\bar{H}D$), $n_{g_i}=3$; if the individual was not hospitalised and died ($g_i=\bar{H}\bar{D}$), $n_{g_i}=2$.

We are interested in inferring the timing of the different steps of the healthcare pathways depending on the group $g_{i}$. To fully describe the healthcare pathway, we need to define the onset of the disease (no other date should be anterior) and then $n_{g_i}-1$ delays to reach the $n_{g_i}-1$ remaining enpoints. Any pairs of points can be chosen, providing that there exists a path from the origin to the endpoints. For example, in the case of the group $HD$, (onset -> hospitalisation, hospitalisation -> death, onset -> report) is a valid representation, while (onset -> hospitalisation, hospitalisation -> Death, onset -> report)
$\Delta_i^k$ the delay taken by individual $i$ for the $k$th delay distribution 

We assume that each data (here date) can have been recorded with error, or can be missing in the observations. Hence we introduce augmented data $D_i^k$ $(k=1, \ldots, n_{g_i})$, where $D_i^k$ is the true data corresponding to the observed data $y_i^k$. 

## Likelihood

### Observation level

The data from the healthcare pathways can be missing or recorded, and, if recorded, can be correct or erroneous. To each observed data entry $y_i^k$ is thus associated an indicator function $E_i^k \in \{-1,0,1 \}$ (missing, recorded and no error, recorded and erroneous). We have
$$P(E_i^k=1|E_i^k \neq -1)= Bernoulli(\zeta)$$
with $\zeta$ the error probability. The likelihood associated with the observed data $y_i^k$ conditional on the true data $D_i^k$ is thus:
$$P(y_i^k|D_i^k,E_i^k=0)=\delta_{y_i^k,D_i^k}$$
$$P(y_i^k|D_i^k,E_i^k=1)=h_{\theta_h}\left(D_i^k\right)$$
$$P(y_i^k|D_i^k,E_i^k=-1)=1$$

Here we assume that any date between $T$ and $T_{0}$ are equally likely to be recorded and thus $h_{\theta_h}\left(D_i^k\right)=\frac{1}{T-T_{0}}$.

NOTE: discuss what $h_{\theta_h}$ should be; could include conditions such as error on onset date > error on death date. Also think about the space of possible errors - if wider than h is smaller and moves towards E=1 might be difficult to achieve. What happens if T_0 is ridiculously early or T ridiculously late in the dataset - this will affect the likelihood. 

### Difference level

This level describes the difference between two observations: 
for $k\geq 2$, $P\left(D_i^k|D_i^{k-1}, g_i\right) = f_{k-1, k}^{g_i}\left(D_i^{k}-D_i^{k-1}\right)$. 

We use discretised gamma distributions for $f_{k-1, k}^{g_i}$ with mean and coefficient of variation (before discretisation) being noted respectively $\mu_{k-1, k}^{g_i}$ and $CV_{k-1, k}^{g_i}$. 

<!--In our example, we assume that the distribution of the delay between exposure and onset $f_{E, O}^{g_i}=f_{E, O}$ does not depend on the group - NOT RELEVANT FOR NOW AS NOT CONSIDERING EXPOSURE DATE -->

### Full posterior distribution

The joint posteior distribution of parameters and augmented data given observed data is:

$\begin{aligned}
P\left(\zeta, \mu, CV, D, E | g, y\right) \propto& P\left(\zeta, \mu, CV, D, E, g, y\right) \\
  \propto& P\left(y | D, E, \zeta, \mu, CV, g\right) P\left(E | \zeta, D, \mu, CV, g\right) P\left(D | \zeta, \mu, CV, g\right) P\left(\zeta\right) P\left(\mu\right) P\left(CV\right)\\
    \propto& P\left(y | D, E\right) P\left(E | \zeta\right) P\left(D | \zeta, \mu, CV, g\right)P\left(\zeta\right) P\left(\mu\right) P\left(CV\right)
\end{aligned}$

where $P\left(D | \zeta, \mu, CV, g\right) \propto \prod_i \prod_{k=2}^{n_{g_i}} P\left( D_i^k | D_i^{k-1}, \mu, CV, g \right)$ (assuming a uniform prior for $D_i^1$). 

## Priors

We use an informative beta prior for $\zeta$, and uninformative flat exponential priors for $\mu$ and $CV$, the mean and coefficient of variation of all delays. 

## Moves

TO WRITE BETTER
Metropolis algorithm with Lognormal proposal for $\mu$, $CV$
Beta Gibbs sampler for $\zeta$
Independant metropolis algorithm for dates $D$, where for each date $D$ we propose a new value drawn in one of the marginal posterior delays this particular date is involved, with $E$ being automatically updated accordingly. Only a fraction of dates (1/10 at the moment) updated at each iteration. 

## Simulation study

TO DESCRIBE

## Error model

* We assume that the data entries are correct typewise, i.e. they are actual date (so "45/10/2012" is not an option)

* We model the potential error generation by a multinomial with five probabilities corresponding to:
1. Internal swap (transposition), the data entry person (DEP) swap the two figures of the month or day (e.g. 12 instead of 21), probability $\epsilon_{w}$
2. External swap (transposition), day and month subfields are swapped, probability $\epsilon_{b}$
3. Neighbouring typo (transcription), DEP mistype one digit for one nearby on the keyboard, +/- 1 position on the keyboard - to start with, probability $\epsilon_{n}$
4. Random typo (transcription), DEP replace one digit by a non neighbour one while the date remain valid, probability $\epsilon_{R}$
5. Unspecified error, any other error not defined above, probability $1- \sum\limits_{X=\{w, b, n, r\}} \epsilon_{X}$

## Website

* Think about a reward -> "unlock" a donation to MSF?
* Frame the whole operation as the "Typing challenge"
* Promotional film, kickstarter style
* Maria Piggin? public involvement? LSHTM media people? ask about potential for tweet etc of how to reach the maximum amount of people
* Short question about ID and how to grab from the browser information (e.g. keyboard?)
* Option tick for email in order to give feedback
* We will have 5 possible ways of proposing a date
1. Plain writing e.g. "Thursday 24th June 2014"
2. American way e.g. June 24th 2014
3. Month of calendar (image) with a date circle in rate
4. Relative day, e.g, "last Monday"
5. Use image database of handwriting training set to generate a "handwritten" data in the right format (reading error)
* Timer to perturbate people?
* First entry should not be too challenging,
* We should store the response entry and the proposed things to read, we can do regression to see if particular entries are associate with particular error (or simply more errors)

# Plan d'attaque

* Essential
1. Allow different sd for proposal for different delay means and sds
2. Adaptive sd for proposals - independently or jointly? 
3. Speed up - put some code in C

* Non essential
1. Explorer simulations en parallele (Anne)
2. Write some code to restart from last point of the chain (Anne)
3. ??? Where we use ncol(curr_aug_dat$D[[g]]), check this as I think it may need to be defined from index_dates rather than from D (Anne)
4. do we indeed want to update zeta after each D_i move? maybe not useful? 

* More substantial changes
1. Coder le model d'erreur (both)
2. Voir pour l'hebergement (Anne)
3. Code pour le shiny app (Marc)

* Steps toward publication
1. Check double entry data by Pierre's HPRU post-doc [Sarah Chilsom] (Anne)
2. Retrieve original uncleaned ebola dataset (Anne)
3. Think about alternative datasets (Marc)
4. Think of sexy graphs and useful for the SI (both)

[Link to google doc for timelines and tasks](https://docs.google.com/document/d/1VIM5F2AV9co_J4MnCe9fY2A-semD_qk6hbnKWkV2j54/edit?usp=sharing)

[Link to google doc for PPI plan](https://docs.google.com/document/d/1Fq4NtcFz0TD79wT0Y5F73_RGx663KUgYdfwsJoJjgt0/edit?usp=sharing)

[Link to the script for the video](https://docs.google.com/document/d/1YAJc-mBDc4zpRu7BEZtilRj_4U0X2sOE2K_wiHOUv1Y/edit?usp=sharing)


